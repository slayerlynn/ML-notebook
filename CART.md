# CART 决策树模型

## 1. 基本概念
CART(CLASSIFICATION AND REGRESSION TREE)是一种二叉树模型,是有监督的机器学习模型，对于分类和回归两种任务，通过解决最优化问题如MSE、基尼系数、信息增益和信息增益比来实现数据的划分和预测。
- 分类树
  给定特征 𝑋，预测类别标签 𝑦，如（是/否，高/中/低）等标签。
  损失函数主要使用**基尼系数**或**信息增量**。
  
  $$
  Gini = 1 - \sum p_k^2
  $$

  $$
  Gain(D, A) = Entropy(D) - \sum_{v=1}^{V} \frac{|D_v|}{|D|} Entropy(D_v)
  $$
  
- 回归树
  给定特征*X*,预测连续变量*y*。
  损失函数主要使用**MSE**。

$$
MSE=\frac{1}{V} \sum_{v=1}^{V}(y_v-\hat{y}_v)^2
$$

## 2. 损失函数
- 基尼系数（二分类）

$$
Gini = 1 - \sum p_k^2
$$

GAIN = 
- sigmoid(二分类）

1.Sigmoid函数

$$p(x) = \sigma(f(x)) = \frac{1}{1 + e^{-f(x)}}$$

2. 各符号含义解析

| 符号          | 名称/含义                                                                 |
|---------------|--------------------------------------------------------------------------|
| $\sigma(\cdot)$ | Sigmoid激活函数符号，本质是一种“压缩函数”                                 |
| $p(x)$         | 函数输出值，范围为 **[0, 1]**，常用于二分类任务中表示“样本属于正类的概率” |
| $f(x)$         | 线性回归模型的输出，计算公式为：<br>$f(x) = w_1x_1 + w_2x_2 + ... + w_nx_n + b$ |
| $w_n$          | 特征权重，对应第$n$个特征$x_n$的重要性系数                               |
| $x_n$          | 模型输入的第$n$个特征（如数据集中的单个属性）                            |
| $b$            | 偏置项（截距），用于调整模型的基础输出水平，避免仅依赖特征值              |
| $e$            | 自然常数，约等于2.718，此处通过$e^{-f(x)}$实现对输入值的范围压缩        |

3. 函数关键特性
- **取值范围固定**：无论输入$x$是何值，输出$p(x)$始终落在 **0 到 1** 之间，这一特性使其天然适用于“概率输出”场景（如判断邮件是否为垃圾邮件的概率）。
- **单调递增关系**：当输入$f(x)$增大时，输出$p(x)$会随之增大；当$f(x)$减小时，$p(x)$也会减小。且当$f(x) = 0$时，$p(x) = 0.5$（可作为二分类的“决策边界”）。
- **梯度特性与局限**：在$f(x)$接近0（即$p(x)$接近0.5）时，函数梯度较大，模型训练效率高；但当$f(x)$趋近于$+\infty$（$p(x)$接近1）或$-\infty$（$p(x)$接近0）时，梯度会趋近于0，导致“梯度消失”问题，可能使模型训练停滞。
- MSE（回归）

$$
MSE=\frac{1}{V} \sum_{v=1}^{V}(y_v-\hat{y}_v)^2
$$
  
- 交叉熵+softmax(多分类）
  

## 3. 构建流程
1. 选择最优划分点
2. 递归生成子树
3. 剪枝优化


